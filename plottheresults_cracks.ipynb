{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all the Output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "outputfolderLoc = r\"/home/dcil/Desktop/Abhi/CrackDetection/ExcelData/Output/Manhatten_Batch1_OUT/CORRECTED\"\n",
    "\n",
    "# List to hold individual DataFrames\n",
    "all_csvs = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for file in tqdm(os.listdir(outputfolderLoc)):\n",
    "    if file.endswith(\".csv\"):\n",
    "        csv_path = os.path.join(outputfolderLoc, file)\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            all_csvs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "combined_df = pd.concat(all_csvs, ignore_index=True)\n",
    "\n",
    "# Output file path\n",
    "output_csv_path = os.path.join(outputfolderLoc, \"combined_output.csv\")\n",
    "\n",
    "# Save the combined DataFrame\n",
    "combined_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Combined CSV saved to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot detection with severity red high, green low , yellow mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1 - Processing images: 100%|██████████| 8/8 [00:04<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "contour_csv_path = r\"/home/dcil/Desktop/Abhi/CrackDetection/ExcelData/Output/6files_OUT/CORRECTED/196_8.csv\"\n",
    "metadata_csv_path = r\"/home/dcil/Desktop/Abhi/CrackDetection/ExcelData/Input/6files.csv\"\n",
    "output_folder = r\"/run/user/1000/gvfs/smb-share:server=10.15.1.24,share=video%202/Software department data/SANJAY/OFFICE WORK/Cracks/Outputs/ICC_05_08_2025\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Generate dynamic colors for classes\n",
    "def get_class_colors(n_classes):\n",
    "    cmap = plt.get_cmap('tab20')  # good for up to 20 classes\n",
    "    colors = {}\n",
    "    for i in range(n_classes):\n",
    "        color = cmap(i / n_classes)  # RGBA normalized\n",
    "        bgr = (\n",
    "            int(color[2] * 255),\n",
    "            int(color[1] * 255),\n",
    "            int(color[0] * 255)\n",
    "        )\n",
    "        colors[i] = bgr\n",
    "    return colors\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "df_meta = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "# Parameters\n",
    "chunk_size = 100000\n",
    "missing_images_log = []\n",
    "chunk_idx = 0\n",
    "\n",
    "# Process CSV in chunks\n",
    "for chunk in pd.read_csv(contour_csv_path, chunksize=chunk_size):\n",
    "    chunk_idx += 1\n",
    "    print(f\"Processing chunk {chunk_idx}...\")\n",
    "\n",
    "    # Merge contour data with metadata\n",
    "    df_merged = pd.merge(chunk, df_meta, on=[\"IDSourceFrame\", \"IDSession\"], how=\"left\")\n",
    "\n",
    "    # Group by image identifiers\n",
    "    grouped = df_merged.groupby([\"IDSourceFrame\", \"IDSession\"])\n",
    "    for (id_frame, id_session), group in tqdm(grouped, desc=f\"Chunk {chunk_idx} - Processing images\"):\n",
    "\n",
    "        image_path = group['paths'].iloc[0]\n",
    "        image_name = group['ImageName'].iloc[0]\n",
    "\n",
    "        # Validate image path\n",
    "        if not isinstance(image_path, str) or not os.path.exists(image_path):\n",
    "            missing_images_log.append((id_frame, id_session, image_path))\n",
    "            continue\n",
    "\n",
    "        # Prepare output filename\n",
    "        safe_name = os.path.basename(image_name)\n",
    "        image_parts = image_path.split(\"/\")\n",
    "        saferequiredname = f\"{image_parts[-4]}_{safe_name}.jpg\"\n",
    "        save_path = os.path.join(output_folder, saferequiredname)\n",
    "\n",
    "        # Load or split existing saved image\n",
    "        if os.path.exists(save_path):\n",
    "            saved_image = cv2.imread(save_path)\n",
    "            if saved_image is None:\n",
    "                print(f\"Could not load saved image: {save_path}\")\n",
    "                continue\n",
    "            w = saved_image.shape[1] // 2\n",
    "            imgOriginal = saved_image[:, :w]\n",
    "            image = saved_image[:, w:].copy()\n",
    "            imgHeight = image.shape[0]\n",
    "        else:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                missing_images_log.append((id_frame, id_session, image_path))\n",
    "                continue\n",
    "            imgOriginal = image.copy()\n",
    "            imgHeight = image.shape[0]\n",
    "\n",
    "        # --- Color Mapping Logic ---\n",
    "        fixed_colors = {\n",
    "            0: (0, 255, 0),    # Green\n",
    "            1: (0, 255, 255),  # Yellow\n",
    "            2: (0, 0, 255),    # Red\n",
    "        }\n",
    "\n",
    "        unique_classes = group[\"Class\"].unique()\n",
    "        other_classes = [cls for cls in unique_classes if cls not in fixed_colors]\n",
    "        dynamic_colors = get_class_colors(len(other_classes))\n",
    "\n",
    "        # Combine fixed + dynamic colors\n",
    "        class_to_color = {}\n",
    "        for cls in unique_classes:\n",
    "            if cls in fixed_colors:\n",
    "                class_to_color[cls] = fixed_colors[cls]\n",
    "            else:\n",
    "                idx = other_classes.index(cls)\n",
    "                class_to_color[cls] = dynamic_colors[idx]\n",
    "\n",
    "        # --- Draw contours ---\n",
    "        for contour_id, contour_group in group.groupby(\"ContourID\"):\n",
    "            scaleFactor = 0.003999\n",
    "            X = contour_group[\"X\"].values / scaleFactor\n",
    "            Y = imgHeight - (contour_group[\"Y\"].values / scaleFactor)\n",
    "\n",
    "            class_label = contour_group[\"Class\"].iloc[0]\n",
    "            color = class_to_color.get(class_label, (255, 255, 255))  # fallback white\n",
    "\n",
    "            pts = np.stack([X, Y], axis=1).astype(np.int32).reshape((-1, 1, 2))\n",
    "\n",
    "            if len(pts) >= 2:\n",
    "                cv2.polylines(image, [pts], isClosed=False, color=color, thickness=2)\n",
    "\n",
    "        # Concatenate original + annotated images\n",
    "        hConcatImage = cv2.hconcat([imgOriginal, image])\n",
    "        cv2.imwrite(save_path, hConcatImage)\n",
    "\n",
    "# Save missing image log\n",
    "if missing_images_log:\n",
    "    log_path = os.path.join(output_folder, \"missing_images_log.csv\")\n",
    "    pd.DataFrame(missing_images_log, columns=[\"IDSourceFrame\", \"IDSession\", \"MissingPath\"]).to_csv(log_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "contour_csv_path = r\"/home/dcil/Desktop/Abhi/CrackDetection/ExcelData/Output/Manhatten_Batch1_OUT/RAW/combined_output.csv\"\n",
    "metadata_csv_path = r\"/home/dcil/Desktop/Abhi/CrackDetection/ExcelData/Input/Manhatten_Batch1.csv\"\n",
    "output_folder = r\"/run/user/1000/gvfs/smb-share:server=10.15.1.24,share=video%202/Software department data/SANJAY/OFFICE WORK/Cracks/Outputs/ICC_Manhatten_hconcat_3\"\n",
    "anotherwithseverity = r\"/run/user/1000/gvfs/smb-share:server=10.15.1.24,share=video%202/Software department data/SANJAY/OFFICE WORK/Cracks/Outputs/ICC_Manhatten_hconcat_3\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "df_meta = pd.read_csv(metadata_csv_path)\n",
    "\n",
    "# Parameters\n",
    "chunk_size = 100000\n",
    "missing_images_log = []\n",
    "chunk_idx = 0\n",
    "\n",
    "# Process CSV in chunks\n",
    "for chunk in pd.read_csv(contour_csv_path, chunksize=chunk_size):\n",
    "    chunk_idx += 1\n",
    "    print(f\"Processing chunk {chunk_idx}...\")\n",
    "\n",
    "    # Merge with metadata\n",
    "    df_merged = pd.merge(chunk, df_meta, on=[\"IDSourceFrame\", \"IDSession\"], how=\"left\")\n",
    "\n",
    "    # Progress bar over grouped image data\n",
    "    grouped = df_merged.groupby([\"IDSourceFrame\", \"IDSession\"])\n",
    "    for (id_frame, id_session), group in tqdm(grouped, desc=f\"Chunk {chunk_idx} - Processing images\"):\n",
    "        image_path = group['paths'].iloc[0]\n",
    "        image_name = group['ImageName'].iloc[0]\n",
    "\n",
    "        # Skip if image path is invalid or doesn't exist\n",
    "        if not isinstance(image_path, str) or not os.path.exists(image_path):\n",
    "            missing_images_log.append((id_frame, id_session, image_path))\n",
    "            continue\n",
    "\n",
    "        # Get image name and output filename\n",
    "        safe_name = os.path.basename(image_name)\n",
    "        image_parts = image_path.split(\"/\")\n",
    "        saferequiredname = f\"{image_parts[-4]}_{safe_name}.jpg\"\n",
    "        save_path = os.path.join(output_folder, saferequiredname)\n",
    "\n",
    "        # Check if image is already saved (reopen and draw again)\n",
    "        if os.path.exists(save_path):\n",
    "            saved_image = cv2.imread(save_path)\n",
    "            if saved_image is None:\n",
    "                print(f\"Could not load existing saved image: {save_path}\")\n",
    "                continue\n",
    "\n",
    "            # Split into left and right halves\n",
    "            w = saved_image.shape[1] // 2\n",
    "            imgOriginal = saved_image[:, :w]\n",
    "            image = saved_image[:, w:].copy()\n",
    "        else:\n",
    "            # Load fresh image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                missing_images_log.append((id_frame, id_session, image_path))\n",
    "                continue\n",
    "            imgOriginal = image.copy()\n",
    "\n",
    "        # Draw contours on the right half image\n",
    "        for contour_id, contour_group in group.groupby(\"ContourID\"):\n",
    "            pts = contour_group[['PixelX', 'PixelY']].values.astype(np.int32).reshape((-1, 1, 2))\n",
    "            if len(pts) >= 2:\n",
    "                cv2.polylines(image, [pts], isClosed=False, color=(0, 0, 255), thickness=2)\n",
    "\n",
    "        # Concatenate and save\n",
    "        hConcatImage = cv2.hconcat([imgOriginal, image])\n",
    "        cv2.imwrite(save_path, hConcatImage)\n",
    "\n",
    "# Save missing image logs\n",
    "if missing_images_log:\n",
    "    log_path = os.path.join(output_folder, \"missing_images_log.csv\")\n",
    "    pd.DataFrame(missing_images_log, columns=[\"IDSourceFrame\", \"IDSession\", \"MissingPath\"]).to_csv(log_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trackDetect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
